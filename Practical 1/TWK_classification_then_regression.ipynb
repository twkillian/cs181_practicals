{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook outlines the final model tuning and set of predictions that ML Marauders have made for CS 181 Practical 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and gently process the data (much of the preprocessing was done in FINAL.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('sam_data/rdk_feat_eng_whole_df_train_orig_features.csv')\n",
    "# df_test = pd.read_csv('sam_data/rdk_feat_eng_whole_df_test_orig_features.csv')\n",
    "df_train = pd.read_csv('final_data/FINAL_train.csv')\n",
    "df_test = pd.read_csv('final_data/FINAL_test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using StandardScaler to normalize non-binary columns\n",
    "scaler = StandardScaler()\n",
    "binary_cols = ['feat_%03d' % ii for ii in range(1,257)]\n",
    "binary_cols.append('has_benzothiophene')\n",
    "binary_cols.append('has_carbazole')\n",
    "binary_cols.append('has_fluorene')\n",
    "binary_cols.append('smiles')\n",
    "binary_cols.append('Id')\n",
    "non_binary_cols = np.array([col for col in df_test.columns if col not in binary_cols]).flatten()\n",
    "\n",
    "df_train[non_binary_cols] = scaler.fit_transform(df_train[non_binary_cols])\n",
    "df_test[non_binary_cols] = scaler.transform(df_test[non_binary_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the 'smiles' and 'Id' columns\n",
    "df_train = df_train.drop(['smiles'], axis=1)\n",
    "df_test = df_test.drop(['Id'], axis=1)\n",
    "\n",
    "# Store gap values\n",
    "Y_train = df_train.gap.values\n",
    "\n",
    "# Delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)\n",
    "X_train = df_train.values\n",
    "X_test = df_test.values\n",
    "print \"Train features:\", X_train.shape, \"Train gap:\", Y_train.shape\n",
    "print \"Test features:\", X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets as well as begin some k-fold CV\n",
    "cross_X_train, cross_X_valid, cross_Y_train, cross_Y_valid = train_test_split(X_train, Y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For classification purposes, round target values to nearest .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Round to nearest integer\n",
    "# Y_clf_train, Y_clf_valid = cross_Y_train.round(), cross_Y_valid.round()\n",
    "# Round to nearest .5\n",
    "# Y_clf_train, Y_clf_valid = np.round(2*cross_Y_train)/2.0, np.round(2*cross_Y_valid)/2.0\n",
    "# Round to nearest .25\n",
    "Y_clf_train, Y_clf_valid = np.round(4*cross_Y_train)/4.0, np.round(4*cross_Y_valid)/4.0\n",
    "Y_full_clf_train = np.round(4*Y_train)/4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"'Training' features: \", cross_X_train.shape\n",
    "print \"'Validate' features: \", cross_X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL:\n",
    "\n",
    "This notebook is set-up to chain together classification and regression methods. The thought is that we can, after we've trained the two models, to first apply a classifier to the data (in a clustering kind of sense) and then use the category or neighborhood that the sample is assigned as an additional feature to perform regression. Here the category or label will be the closest integer to the gap value. The idea behind this is to hijack the regression into a local region of the expected HOMO-LUMO gap based on the label. The hope is that this will pin the regressor closer to the right value. \n",
    "\n",
    "It's imperative that we get as accurate of a classifier as we can.\n",
    "\n",
    "Fingers crossed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Let's build a classifier that will adequately label the samples\n",
    "\n",
    "We'll start with Logistic Regression and try to fit the best model using a collection of C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logReg_training_acc = 0\n",
    "logReg_test_acc = 0\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "for c in Cs:\n",
    "    clf_logReg=LogisticRegression(penalty=\"l2\",C=c, solver='lbfgs')\n",
    "    clf_logReg.fit(cross_X_train,Y_clf_train)\n",
    "    training_acc = clf_logReg.score(cross_X_train,Y_clf_train)\n",
    "    test_acc = clf_logReg.score(cross_X_valid,Y_clf_valid)\n",
    "    print c, test_acc\n",
    "    if logReg_test_acc < test_acc:\n",
    "        logReg_test_acc = test_acc\n",
    "        logReg_training_acc = training_acc\n",
    "        best_logReg = clf_logReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logReg_training_acc = best_logReg.score(cross_X_train,Y_clf_train)\n",
    "logReg_test_acc = best_logReg.score(cross_X_valid,Y_clf_valid)\n",
    "print \"Training Accuracy: %0.3f\" % logReg_training_acc\n",
    "print \"Test Accuracy: %0.3f\" % logReg_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate predicted labels onto test/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_clf_pred = best_logReg.predict(cross_X_valid)\n",
    "X_train_clf = np.vstack((cross_X_train.T,Y_clf_train)).T\n",
    "X_valid_clf = np.vstack((cross_X_valid.T,Y_clf_valid)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now onto ExtraTrees, fronted with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pcaExtraTrees_RMSE = 100\n",
    "\n",
    "pca_components = [30, 35, 40, 45]\n",
    "num_estimators = [50, 100, 200]\n",
    "\n",
    "for comps in pca_components:\n",
    "    pca = PCA(n_components=comps)\n",
    "    X_train_tr = pca.fit_transform(X_train_clf)\n",
    "    X_valid_tr = pca.transform(X_valid_clf)\n",
    "    \n",
    "    for n_estimators in num_estimators:\n",
    "        \n",
    "        extratrees_clf = ExtraTreesRegressor(n_estimators=n_estimators,n_jobs=2)\n",
    "        extratrees_clf.fit(X_train_tr,cross_Y_train)\n",
    "        y_pred = extratrees_clf.predict(X_valid_tr)\n",
    "        \n",
    "        RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "        if RMSE < pcaExtraTrees_RMSE:\n",
    "            print comps, n_estimators\n",
    "            print RMSE\n",
    "            pcaExtraTrees_RMSE = RMSE\n",
    "            pcaExtraTrees_estimators = n_estimators\n",
    "            pcaExtraTrees_components = comps\n",
    "            best_pcaExtraTrees = extratrees_clf\n",
    "            \n",
    "print \"PCA with {0} components chained ExtraTrees with {1} estimators had RMSE of {2}\".format(pcaExtraTrees_components,pcaExtraTrees_estimators,pcaExtraTrees_RMSE)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're also going to tune a Ridge Regression to have double coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_RMSE = 100\n",
    "alphas = np.logspace(-4, 1, 30)\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_clf = Ridge(alpha=alpha)\n",
    "    ridge_clf.fit(X_train_clf, cross_Y_train)\n",
    "    y_pred = ridge_clf.predict(X_valid_clf)\n",
    "    \n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "    if RMSE < ridge_RMSE:\n",
    "        ridge_RMSE = RMSE\n",
    "        ridge_alpha = alpha\n",
    "        best_ridge = ridge_clf\n",
    "        \n",
    "print \"Ridge RMSE: {0} with alpha: {1}\".format(ridge_RMSE,ridge_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full training set, run on full test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train the classifier and the regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "best_logReg.fit(X_train,Y_full_clf_train)\n",
    "\n",
    "# Concatenate full training labels to full test set\n",
    "X_full_train_clf = np.vstack((X_train.T,Y_full_clf_train)).T\n",
    "\n",
    "# Train ExtraTrees Regressor\n",
    "pca = PCA(n_components=45)\n",
    "X_full_train_tr = pca.fit_transform(X_full_train_clf)\n",
    "best_pcaExtraTrees.fit(X_full_train_tr,Y_train)\n",
    "\n",
    "# Train Ridge Regression\n",
    "best_ridge.fit(X_full_train_clf,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run classifier on full test set (get category assignments) and concatenate feature to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run classifier on test set\n",
    "label_pred = best_logReg.predict(X_test)\n",
    "\n",
    "# Concatenate predicted labels onto test set as a new feature\n",
    "X_test_clf = np.vstack((X_test.T,label_pred)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the two regressions on the augmented test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run ExtraTrees Regressor\n",
    "pcaExtraTrees_pred = best_pcaExtraTrees.predict(X_test_clf)\n",
    "\n",
    "# Run Ridge Regressor\n",
    "ridge_pred = best_ridge.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the ExtraTrees Predictions\n",
    "write_to_file(\"pca_extraTrees_FINAL_TWK_10Feb.csv\",pcaExtraTrees_pred)\n",
    "\n",
    "# Save the Ridge Predictions\n",
    "write_to_file(\"ridge_FINAL_TWK_10Feb.csv\",ridge_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
