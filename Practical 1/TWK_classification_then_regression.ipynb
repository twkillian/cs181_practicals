{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook outlines the final model tuning and set of predictions that ML Marauders have made for CS 181 Practical 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge, LogisticRegression, LogisticRegressionCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and gently process the data (much of the preprocessing was done in FINAL.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('sam_data/rdk_feat_eng_whole_df_train_orig_features.csv')\n",
    "# df_test = pd.read_csv('sam_data/rdk_feat_eng_whole_df_test_orig_features.csv')\n",
    "# df_train = pd.read_csv('final_data/FINAL_train.csv')\n",
    "# df_test = pd.read_csv('final_data/FINAL_test.csv')\n",
    "df_train = pd.read_csv('FINAL_interactions/FINAL_train_25_interactions.csv')\n",
    "df_test = pd.read_csv('FINAL_interactions/FINAL_test_25_interactions.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the 'smiles' and 'Id' columns\n",
    "df_train = df_train.drop(['smiles'], axis=1)\n",
    "df_test = df_test.drop(['Id'], axis=1)\n",
    "df_test = df_test.drop(['smiles'], axis=1)\n",
    "\n",
    "# Store gap values\n",
    "Y_train = df_train.gap.values\n",
    "\n",
    "# Delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)\n",
    "X_train = df_train.values\n",
    "X_test = df_test.values\n",
    "print \"Train features:\", X_train.shape, \"Train gap:\", Y_train.shape\n",
    "print \"Test features:\", X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets as well as begin some k-fold CV\n",
    "cross_X_train, cross_X_valid, cross_Y_train, cross_Y_valid = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For classification purposes, round target values to nearest .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Round to nearest integer\n",
    "# cross_Y_train_labels, cross_Y_valid_labels = np.round(cross_Y_train), np.round(cross_Y_valid)\n",
    "# Y_train_labels = np.round(Y_train)\n",
    "# Round to nearest .5\n",
    "cross_Y_train_labels, cross_Y_valid_labels = (((np.round(2*cross_Y_train)/2.0)-0.5)/0.5).astype(int), (((np.round(2*cross_Y_valid)/2.0)-0.5)/0.5).astype(int)\n",
    "Y_train_labels = (((np.round(2*Y_train)/2.0)-.5)/.5).astype(int)\n",
    "# Round to nearest .25\n",
    "# cross_Y_train_labels, cross_Y_valid_labels = (((np.round(4*cross_Y_train)/4.0)-.25)/.25).astype(int), (((np.round(4*cross_Y_valid)/4.0)-.25)/.25).astype(int)\n",
    "# Y_train_labels = (((np.round(4*Y_train)/4.0)-.25)/.25).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"'Training' features: \", cross_X_train.shape\n",
    "print \"'Validate' features: \", cross_X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL:\n",
    "\n",
    "This notebook is set-up to chain together classification and regression methods. The thought is that we can, after we've trained the two models, to first apply a classifier to the data (in a clustering kind of sense) and then use the category or neighborhood that the sample is assigned as an additional feature to perform regression. Here the category or label will be the closest integer to the gap value. The idea behind this is to hijack the regression into a local region of the expected HOMO-LUMO gap based on the label. The hope is that this will pin the regressor closer to the right value. \n",
    "\n",
    "It's imperative that we get as accurate of a classifier as we can.\n",
    "\n",
    "Fingers crossed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Let's build a classifier that will adequately label the samples\n",
    "\n",
    "We'll start with Logistic Regression and try to fit the best model using a collection of C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logReg_training_acc = 0\n",
    "logReg_test_acc = 0\n",
    "\n",
    "Cs = [0.01, 0.1, 1.0]\n",
    "\n",
    "for c in Cs:\n",
    "    clf_logReg=LogisticRegression(penalty=\"l2\",C=c, solver='lbfgs')\n",
    "    clf_logReg.fit(cross_X_train,cross_Y_train_labels)\n",
    "    training_acc = clf_logReg.score(cross_X_train,cross_Y_train_labels)\n",
    "    test_acc = clf_logReg.score(cross_X_valid,cross_Y_valid_labels)\n",
    "    print c, test_acc\n",
    "    if logReg_test_acc < test_acc:\n",
    "        logReg_test_acc = test_acc\n",
    "        logReg_training_acc = training_acc\n",
    "        best_logReg = clf_logReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logReg_training_acc = best_logReg.score(cross_X_train,Y_clf_train)\n",
    "logReg_test_acc = best_logReg.score(cross_X_valid,Y_clf_valid)\n",
    "print \"Training Accuracy: %0.3f\" % logReg_training_acc\n",
    "print \"Test Accuracy: %0.3f\" % logReg_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate predicted labels onto test/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adding_labels(feat_matrix,labels):\n",
    "    '''Helper function that creates sparse binary array to concatenate to feat_matrix'''\n",
    "    # Create empty matrix \n",
    "    added_cols = np.zeros((labels.shape[0],np.unique(labels).shape[0]))\n",
    "    # Increment entry that corresponds to the sample having the specified label\n",
    "    for ii in xrange(labels.shape[0]):\n",
    "        added_cols[ii,labels[ii]] = 1\n",
    "    # Concatenate label columns to feat_matrix\n",
    "    feat_matrix = np.concatenate((feat_matrix,added_cols),axis=1)\n",
    "    return feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_X_train = adding_labels(cross_X_train,cross_Y_train_labels)\n",
    "\n",
    "cross_X_valid = adding_labels(cross_X_valid,cross_Y_valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now generating a Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ExtraTrees_RMSE = 100\n",
    "num_estimators = [10, 64, 96, 128]\n",
    "\n",
    "    \n",
    "for n_estimators in num_estimators:\n",
    "\n",
    "    rf_reg = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1)\n",
    "    rf_reg.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = rf_reg.predict(cross_X_valid)\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "    if RMSE < ExtraTrees_RMSE:\n",
    "        print n_estimators, RMSE\n",
    "        RF_RMSE = RMSE\n",
    "        RF_estimators = n_estimators\n",
    "        best_RF = rf_reg\n",
    "            \n",
    "print \"RandomForest with {0} estimators had RMSE of {1}\".format(RF_estimators,RF_RMSE)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're also going to tune a typical Linear Regression to have double coverage (either Ridge or Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso_RMSE = 100\n",
    "alphas = np.logspace(-4, 1, 30)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso_reg = Lasso(alpha=alpha)\n",
    "    lasso_reg.fit(cross_X_train, cross_Y_train)\n",
    "    y_pred = lasso_reg.predict(cross_X_valid)\n",
    "    \n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "    if RMSE < lasso_RMSE:\n",
    "        lasso_RMSE = RMSE\n",
    "        lasso_alpha = alpha\n",
    "        best_lasso = lasso_reg\n",
    "        \n",
    "print \"Lasso RMSE: {0} with alpha: {1}\".format(lasso_RMSE,lasso_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ridge_RMSE = 100\n",
    "# alphas = np.logspace(-4, 1, 30)\n",
    "\n",
    "# for alpha in alphas:\n",
    "#     ridge_clf = Ridge(alpha=alpha)\n",
    "#     ridge_clf.fit(X_train_clf, cross_Y_train)\n",
    "#     y_pred = ridge_clf.predict(X_valid_clf)\n",
    "    \n",
    "#     RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "#     if RMSE < ridge_RMSE:\n",
    "#         ridge_RMSE = RMSE\n",
    "#         ridge_alpha = alpha\n",
    "#         best_ridge = ridge_clf\n",
    "        \n",
    "# print \"Ridge RMSE: {0} with alpha: {1}\".format(ridge_RMSE,ridge_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('final2_classifier_and_regressors.pkl','w') as f:\n",
    "    pickle.dump((best_logReg, best_RF, best_lasso),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('final_classifier_and_regressors.pkl','r') as fopen:\n",
    "#     best_logReg, best_pcaExtraTrees, pcaExtraTrees_components, best_ridge = pickle.load(fopen)\n",
    "    \n",
    "# After opening this, you may need to re-configure the test and training set, that is if you have to restart the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full training set, run on full test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train the classifier and the regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train classifier\n",
    "best_logReg.fit(X_train,Y_train_labels)\n",
    "\n",
    "# Run classifier on test set\n",
    "label_pred = best_logReg.predict(X_test)\n",
    "\n",
    "# Concatenate full training labels to full test set\n",
    "X_train_clf = adding_labels(X_train,Y_train_labels)\n",
    "\n",
    "# Concatenate predicted labels onto test set as a new feature\n",
    "X_test_clf = adding_labels(X_test,label_pred)\n",
    "\n",
    "print \"Completed Classification of test set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train ExtraTrees Regressor\n",
    "best_RF.fit(X_train_clf,Y_train)\n",
    "\n",
    "# Run ExtraTrees Regressor\n",
    "RF_pred = best_RF.predict(X_test_clf)\n",
    "\n",
    "# Save the ExtraTrees Predictions\n",
    "write_to_file(\"extraTrees_FINAL2_TWK_10Feb.csv\", RF_pred)\n",
    "\n",
    "print \"Completed Random Forest Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso/Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Train Ridge Regression\n",
    "# best_ridge.fit(X_full_train_clf,Y_train)\n",
    "\n",
    "# # Run Ridge Regressor\n",
    "# ridge_pred = best_ridge.predict(X_test_clf)\n",
    "\n",
    "# # Save the Ridge Predictions\n",
    "# write_to_file(\"ridge_FINAL_TWK_10Feb.csv\",ridge_pred)\n",
    "# print \"Completed Ridge Regression\"\n",
    "\n",
    "# Train Lasso Regression\n",
    "best_lasso.fit(X_train_clf,Y_train)\n",
    "\n",
    "# Run Lasso Regressor\n",
    "lasso_pred = best_lasso.predict(X_test_clf)\n",
    "\n",
    "# Save the Lasso Predictions\n",
    "write_to_file(\"lasso_FINAL2_TWK_10Feb.csv\",lasso_pred)\n",
    "print \"Completed Lasso Regression\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
