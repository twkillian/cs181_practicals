{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoLarsCV, LassoCV, ElasticNetCV, SGDRegressor, Ridge, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: (999997, 282)\n",
      "Train gap: (999997,)\n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "df_train = pd.read_csv('sam_data/rdk_feat_eng_whole_df_train_orig_features.csv')\n",
    "\n",
    "# Drop the 'smiles' column \n",
    "df_train = df_train.drop(['smiles'], axis=1)\n",
    "\n",
    "# Store gap values\n",
    "Y_train = df_train.gap.values\n",
    "\n",
    "# Delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)\n",
    "X_train = df_train.values\n",
    "print \"Train features:\", X_train.shape\n",
    "print \"Train gap:\", Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets as well as begin some k-fold CV\n",
    "cross_X_train, cross_X_valid, cross_Y_train, cross_Y_valid = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (669997, 282)\n",
      "Validation set size: (330000, 282)\n"
     ]
    }
   ],
   "source": [
    "print \"Training set size:\", cross_X_train.shape\n",
    "print \"Validation set size:\", cross_X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SELECTION\n",
    "\n",
    "Here I'm going to work through many of the same models that I had done before (in TWK_hacking.ipynb) but with a lot more care. I'm excited to nail down some great models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 1, 30)\n",
    "pca_components = [10,30,45,60,100]\n",
    "num_estimators = [50,100,200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoLarsCV\n",
    "\n",
    "This model performs cross validation, it determines the best and most relevant alphas by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.821e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.812e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.812e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=8.778e-05, previous alpha=8.728e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.773e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.765e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.765e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=8.730e-05, previous alpha=8.681e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.834e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.825e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.825e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=8.790e-05, previous alpha=8.742e-05, with an active set of 8 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLars best alpha: 8.77773703496e-05\n"
     ]
    }
   ],
   "source": [
    "lassoLars_clf = LassoLarsCV()\n",
    "lassoLars_clf.fit(cross_X_train,cross_Y_train)\n",
    "print \"LassoLars best alpha: {0}\".format(lassoLars_clf.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLars RMSE: 0.305539702757\n"
     ]
    }
   ],
   "source": [
    "y_pred = lassoLars_clf.predict(cross_X_valid) \n",
    "    \n",
    "# Calculate RMSE and update minimum RMSE if possible\n",
    "LassoLars_RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "print \"LassoLars RMSE: {}\".format(LassoLars_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoCV\n",
    "\n",
    "Regular LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso best alpha: 0.0108950693091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso_clf = LassoCV()\n",
    "lasso_clf.fit(cross_X_train,cross_Y_train)\n",
    "print \"Lasso best alpha: {}\".format(lasso_clf.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso RMSE: 0.251127479727\n"
     ]
    }
   ],
   "source": [
    "y_pred = lasso_clf.predict(cross_X_valid)\n",
    "\n",
    "# Calculate RMSE\n",
    "Lasso_RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "print \"Lasso RMSE: {}\".format(Lasso_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "The idea with SGD is that it minimizes empirical loss by following the path that minimizes the gradient by some learning rate (usually $\\propto \\alpha$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD minimized with alpha: 100, resulting in RMSE of: 100\n"
     ]
    }
   ],
   "source": [
    "SGD_alpha = 100\n",
    "SGD_RMSE = 100\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Fit model and predict on validation\n",
    "    sgd_clf = SGDRegressor(alpha=alpha)\n",
    "    sgd_clf.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = sgd_clf.predict(cross_X_valid)\n",
    "    \n",
    "    # Calculate RMSE and update minimum RMSE if possible\n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "    if RMSE < SGD_RMSE:\n",
    "        SGD_RMSE = RMSE\n",
    "        SGD_alpha = alpha\n",
    "        best_sgd = sgd_clf\n",
    "print \"SGD minimized with alpha: {0}, resulting in RMSE of: {1}\".format(SGD_alpha,SGD_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was a disappointment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Gradient Boosting Regression\n",
    "Similar in nature to how SGD works. It's main identifying feature is that it fits a regression tree at each stage of the gradient minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f3622c4e5ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgradBoost_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgradBoost_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_Y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradBoost_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_X_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1025\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1078\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1079\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 784\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ta24418/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gradBoost_RMSE = 100\n",
    "\n",
    "for n_estimators in num_estimators:\n",
    "    print n_estimators\n",
    "    gradBoost_clf = GradientBoostingRegressor(n_estimators=n_estimators)\n",
    "    gradBoost_clf.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = gradBoost_clf.predict(cross_X_valid)\n",
    "    \n",
    "    # Calculate RMSE and update minimum RMSE if possible\n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "    if RMSE < gradBoost_RMSE:\n",
    "        print RMSE\n",
    "        gradBoost_RMSE = RMSE\n",
    "        gradBoost_estimators = n_estimators\n",
    "        best_gradBoost = gradBoost_clf\n",
    "print \"Gradient Boosting minimized with {0} estimators, resulting in RMSE of: {1}\".format(gradBoost_estimators,gradBoost_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + Extra Trees Regressor\n",
    "Just because this is what was giving us the best performance before. I want to verify that this will rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 50\n",
      "0.160239044476\n",
      "10 100\n",
      "0.160199866777\n",
      "10 200\n",
      "0.160130264909\n",
      "30 50\n",
      "0.158392613016\n",
      "30 100\n",
      "0.15829074062\n",
      "30 200\n",
      "0.158224597522\n",
      "45 50\n",
      "0.157385975105\n",
      "45 100\n",
      "0.157234455968\n",
      "45 200\n",
      "0.15717270597\n",
      "60 50\n",
      "60 100\n",
      "60 200\n",
      "0.157171712015\n",
      "100 50\n",
      "100 100\n",
      "100"
     ]
    }
   ],
   "source": [
    "pcaExtraTrees_RMSE = 100\n",
    "\n",
    "for comps in pca_components:\n",
    "    pca = PCA(n_components=comps)\n",
    "    X_train_tr = pca.fit_transform(cross_X_train)\n",
    "    X_valid_tr = pca.transform(cross_X_valid)\n",
    "    \n",
    "    for n_estimators in num_estimators:\n",
    "        \n",
    "        print comps, n_estimators\n",
    "        \n",
    "        extratrees_clf = ExtraTreesRegressor(n_estimators=n_estimators,n_jobs=2)\n",
    "        extratrees_clf.fit(X_train_tr,cross_Y_train)\n",
    "        y_pred = extratrees_clf.predict(X_valid_tr)\n",
    "        \n",
    "        RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "        if RMSE < pcaExtraTrees_RMSE:\n",
    "            print RMSE\n",
    "            pcaExtraTrees_RMSE = RMSE\n",
    "            pcaExtraTrees_estimators = n_estimators\n",
    "            pcaExtraTrees_components = comps\n",
    "            best_pcaExtraTrees = extratrees_clf\n",
    "            \n",
    "print \"PCA with {0} components chained ExtraTrees with {1} estimators had RMSE of {2}\".format(pcaExtraTrees_components,pcaExtraTrees_estimators,pcaExtraTrees_RMSE)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "Inject a little regularization into this beast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_RMSE = 100\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_clf = Ridge(alpha=alpha)\n",
    "    ridge_clf.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = ridge_clf.predict(cross_X_valid)\n",
    "    \n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "    if RMSE < ridge_RMSE:\n",
    "        ridge_RMSE = RMSE\n",
    "        ridge_alpha = alpha\n",
    "        best_ridge = ridge_clf\n",
    "        \n",
    "print \"Ridge RMSE: {0} with alpha: {1}\".format(ridge_RMSE,ridge_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression\n",
    "I dunno if this is going to work or if it's going to be worth the effort and run-time... Just exhausting my options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1]\n",
    "epsilons = [0.075, 0.1, 0.125]\n",
    "\n",
    "SVR_RMSE = 100\n",
    "\n",
    "for comps in pca_components[:3]\n",
    "    \n",
    "    pca = PCA(n_components=comps)\n",
    "    X_train_tr = pca.fit_transform(cross_X_train)\n",
    "    X_valid_tr = pca.transform(cross_X_valid)\n",
    "    \n",
    "    for c in Cs:\n",
    "        for eps in epsilons:\n",
    "        \n",
    "            svr_clf = SVR(C=c,epsilon=eps)\n",
    "            svr_clf.fit(X_train_tr,cross_Y_train)\n",
    "            y_pred = svr_clf.predict(X_valid_tr)\n",
    "            \n",
    "            RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "            if RMSE < SVR_RMSE:\n",
    "                print comps, c, eps\n",
    "                SVR_RMSE = RMSE\n",
    "                SVR_comps = comps\n",
    "                SVR_C = c\n",
    "                SVR_eps = eps\n",
    "                best_SVR = svr_clf\n",
    "print \"SVR RMSE: {0} with C: {1} epsilon {2} and under {3} components\".format(SVR_RMSE,SVR_C,SVR_eps,SVR_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_ratios = [.1, .5, .7, .9, .99]\n",
    "elasticNet_RMSE = 100\n",
    "for comps in pca_components:\n",
    "    pca = PCA(n_components=comps)\n",
    "    X_train_tr = pca.fit_transform(cross_X_train)\n",
    "    X_valid_tr = pca.transform(cross_X_valid)\n",
    "    for ratio in l1_ratios:\n",
    "        print comps, ratio\n",
    "        elasticNet_clf = ElasticNetCV(l1_ratio=ratio,n_jobs=3)\n",
    "        elasticNet_clf.fit(X_train_tr,cross_Y_train)\n",
    "        y_pred = elasticNet_clf.predict(X_valid_tr)\n",
    "        \n",
    "        RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "        if RMSE < elasticNet_RMSE:\n",
    "            print RMSE\n",
    "            elasticNet_RMSE = RMSE\n",
    "            elasticNet_comps = comps\n",
    "            elasticNet_l1ratio = ratio\n",
    "            best_elasticNet = elasticNet_clf\n",
    "\n",
    "print \"ElasticNet RMSE: {0} with L1-ratio: {1} under {2}\".format(elasticNet_RMSE,elasticNet_l1ratio,elasticNet_comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha1s = [1e-07,1e-06,1e-05]\n",
    "alpha2s = [1e-07,1e-06,1e-05]\n",
    "lambda1s = [1e-07,1e-06,1e-05]\n",
    "lambda2s = [1e-07,1e-06,1e-05]\n",
    "bayesRidge_RMSE = 100\n",
    "\n",
    "for alpha1 in alpha1s:\n",
    "    for alpha2 in alpha2s:\n",
    "        for lambda1 in lambda1s:\n",
    "            for lambda2 in lambda2s:\n",
    "                \n",
    "                bayesRidge_clf = BayesianRidge(alpha_1=alpha1,alpha_2=alpha2,lambda_1=lambda1,lambda_2=lambda2)\n",
    "                bayesRidge_clf.fit(cross_X_train,cross_Y_train)\n",
    "                y_pred = bayesRidge.predict(cross_X_valid)\n",
    "                \n",
    "                RMSE = np.sqrt(mean_squared_error(cross_Y_valid,y_pred))\n",
    "                if RMSE < bayesRidge_RMSE:\n",
    "                    bayesRidge_RMSE = RMSE\n",
    "                    bayesRidge_alpha1 = alpha1\n",
    "                    bayesRidge_alpha2 = alpha2\n",
    "                    bayesRidge_lambda1 = lambda1\n",
    "                    bayesRidge_lambda2 = lambda2\n",
    "                    best_bayesRidge = bayesRidge_clf\n",
    "print \"BayesianRidge RMSE: {}\".format(bayesRidge_RMSE)\n",
    "print bayesRidge_alpha1, bayesRidge_alpha2, bayesRidge_lambda1, bayesRidge_lambda2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
