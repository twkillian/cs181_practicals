{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoLarsCV, LassoCV, ElasticNetCV, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: (999997, 282)\n",
      "Train gap: (999997,)\n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "df_train = pd.read_csv('sam_data/rdk_feat_eng_whole_df_train_orig_features.csv')\n",
    "\n",
    "# Drop the 'smiles' column \n",
    "df_train = df_train.drop(['smiles'], axis=1)\n",
    "\n",
    "# Store gap values\n",
    "Y_train = df_train.gap.values\n",
    "\n",
    "# Delete 'gap' column\n",
    "df_train = df_train.drop(['gap'], axis=1)\n",
    "X_train = df_train.values\n",
    "print \"Train features:\", X_train.shape\n",
    "print \"Train gap:\", Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets as well as begin some k-fold CV\n",
    "cross_X_train, cross_X_valid, cross_Y_train, cross_Y_valid = train_test_split(X_train, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (669997, 282)\n",
      "Validation set size: (330000, 282)\n"
     ]
    }
   ],
   "source": [
    "print \"Training set size:\", cross_X_train.shape\n",
    "print \"Validation set size:\", cross_X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SELECTION\n",
    "\n",
    "Here I'm going to work through many of the same models that I had done before (in TWK_hacking.ipynb) but with a lot more care. I'm excited to nail down some great models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, 1, 30)\n",
    "pca_components = [10,15,30,45,60,100,150]\n",
    "num_estimators = [50,100,200,300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoLarsCV\n",
    "\n",
    "This model performs cross validation, it determines the best and most relevant alphas by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLars best alpha: 2.91116456737e-07\n"
     ]
    }
   ],
   "source": [
    "lassoLars_clf = LassoLarsCV()\n",
    "lassoLars_clf.fit(cross_X_train,cross_Y_train)\n",
    "print \"LassoLars best alpha: {0}\".format(lassoLars_clf.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLars RMSE: 0.400655750739\n"
     ]
    }
   ],
   "source": [
    "y_pred = lassoLars_clf.predict(cross_X_valid) \n",
    "    \n",
    "# Calculate RMSE and update minimum RMSE if possible\n",
    "LassoLars_RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "print \"LassoLars RMSE: {}\".format(LassoLars_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoCV\n",
    "\n",
    "Regular LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_clf = LassoCV()\n",
    "lasso_clf.fit(cross_X_train,cross_Y_train)\n",
    "print \"Lasso best alpha: {}\".format(lasso_clf.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lasso_clf.predict(cross_X_valid)\n",
    "\n",
    "# Calculate RMSE\n",
    "Lasso_RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "print \"Lasso RMSE: {}\".format(Lasso_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "The idea with SGD is that it minimizes empirical loss by following the path that minimizes the gradient by some learning rate (usually $\\propto \\alpha$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD minimized with alpha: 100, resulting in RMSE of: 100\n"
     ]
    }
   ],
   "source": [
    "SGD_alpha = 100\n",
    "SGD_RMSE = 100\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Fit model and predict on validation\n",
    "    sgd_clf = SGDRegressor(alpha=alpha)\n",
    "    sgd_clf.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = sgd_clf.predict(cross_X_valid)\n",
    "    \n",
    "    # Calculate RMSE and update minimum RMSE if possible\n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "    if RMSE < SGD_RMSE:\n",
    "        SGD_RMSE = RMSE\n",
    "        SGD_alpha = alpha\n",
    "        best_sgd = sgd_clf\n",
    "print \"SGD minimized with alpha: {0}, resulting in RMSE of: {1}\".format(SGD_alpha,SGD_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was a disappointment..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Gradient Boosting Regression\n",
    "Similar in nature to how SGD works. It's main identifying feature is that it fits a regression tree at each stage of the gradient minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradBoost_RMSE = 100\n",
    "\n",
    "for n_estimators in num_estimators:\n",
    "\n",
    "    gradBoost_clf = GradientBoostingRegressor(n_estimators=n_estimators)\n",
    "    gradBoost_clf.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = gradBoost_clf.predict(cross_X_valid)\n",
    "    \n",
    "    # Calculate RMSE and update minimum RMSE if possible\n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "    if RMSE < gradBoost_RMSE:\n",
    "        gradBoost_RMSE = RMSE\n",
    "        gradBoost_estimators = n_estimators\n",
    "        best_gradBoost = gradBoost_clf\n",
    "print \"Gradient Boosting minimized with {0} estimators, resulting in RMSE of: {1}\".format(gradBoost_estimators,gradBoost_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + Extra Trees Regressor\n",
    "Just because this is what was giving us the best performance before. I want to verify that this will rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set parameters to test\n",
    "alphas = np.logspace(-4, -.5, 30)\n",
    "\n",
    "# Initialize minimums \n",
    "\n",
    "minimum_alpha = 100\n",
    "minimum_RMSE = 100\n",
    "for alpha in alphas:\n",
    "    \n",
    "    # Fit model and predict on validation\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf.fit(cross_X_train,cross_Y_train)\n",
    "    y_pred = clf.predict(cross_X_valid) \n",
    "    \n",
    "    # Calculate RMSE and update minimum RMSE if possible\n",
    "    RMSE = np.sqrt(mean_squared_error(cross_Y_valid, y_pred))\n",
    "    if RMSE < minimum_RMSE:\n",
    "        minimum_RMSE = RMSE\n",
    "        minimum_alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
